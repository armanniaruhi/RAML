{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ced664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing.dataLoader_CelebA import get_partitioned_dataloaders\n",
    "from src.ml.own_network import SiameseNetworkOwn\n",
    "from src.ml.resNet18 import SiameseNetwork\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import yaml\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815a1a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63979669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config from YAML\n",
    "with open(\"config/config.yml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Extract sections\n",
    "PRE = config[\"PREPROCESSING\"]\n",
    "\n",
    "# Set constants from preprocessing config\n",
    "IMAGE_DIR = PRE[\"image_dir\"]\n",
    "LABEL_FILE = PRE[\"label_file\"]\n",
    "PARTITION_FILE = PRE[\"partition_file\"]\n",
    "BATCH_SIZE = PRE[\"batch_size\"] #16\n",
    "M_PER_SAMPLE = PRE[\"m_per_sample\"]\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aac6212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all(train_loader):\n",
    "    # Assuming train_loader uses FilteredCelebADataset with remapped labels\n",
    "\n",
    "    # Access the underlying dataset from train_loader\n",
    "    train_dataset = train_loader.dataset\n",
    "\n",
    "    # Collect one example image per unique label\n",
    "    unique_labels = sorted(set(train_dataset.labels))\n",
    "    label_to_img = {}\n",
    "\n",
    "    for idx, label in enumerate(train_dataset.labels):\n",
    "        if label not in label_to_img:\n",
    "            label_to_img[label] = train_dataset[idx][0]  # Assuming dataset[idx] returns (image, label)\n",
    "\n",
    "    # Plot all unique labels with their example images\n",
    "    num_labels = len(unique_labels)\n",
    "    cols = 10\n",
    "    rows = (num_labels + cols - 1) // cols\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols*2, rows*2))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        img = label_to_img[label].permute(1, 2, 0).numpy()\n",
    "\n",
    "        # Denormalize (if your dataset applies normalization)\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        img = img * std + mean\n",
    "        img = img.clip(0, 1)\n",
    "\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f\"Label: {label}\")\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for j in range(i+1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e339df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader, val_loader, test_loader = get_partitioned_dataloaders(\n",
    "    image_dir=IMAGE_DIR,\n",
    "    label_file=LABEL_FILE,\n",
    "    partition_file=PARTITION_FILE,\n",
    "    m_per_sample=M_PER_SAMPLE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_identities=200,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011a636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "\n",
    "for i in range(5):\n",
    "    sample = test_loader.dataset[i]\n",
    "    image, label = sample[0], sample[3]  # image is first, label is fifth\n",
    "\n",
    "    # Convert image to numpy\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        if image.ndim == 3 and image.shape[0] == 1:  # grayscale\n",
    "            image = image.squeeze().numpy()\n",
    "            cmap = 'gray'\n",
    "        elif image.ndim == 3 and image.shape[0] == 3:  # RGB\n",
    "            image = image.permute(1, 2, 0).numpy()\n",
    "            cmap = None\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected image shape: {image.shape}\")\n",
    "    else:\n",
    "        cmap = 'gray'\n",
    "\n",
    "    # Convert label to int\n",
    "    if isinstance(label, torch.Tensor):\n",
    "        label = label.item()\n",
    "\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.imshow(image, cmap=cmap)\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a7b063",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches_val = len(val_loader)\n",
    "print(f\"Number of batches in validation dataloader: {num_batches_val}\")\n",
    "num_samples_train = len(train_loader.dataset)\n",
    "num_batches_train = len(train_loader)\n",
    "print(f\"Training samples: {num_samples_train}, Batches: {num_batches_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b45ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if \"RESNET\" in MODEL_PATH:\n",
    "    net = SiameseNetwork().to(DEVICE)\n",
    "elif \"OWN\" in MODEL_PATH:\n",
    "    net = SiameseNetworkOwn().to(DEVICE)\n",
    "net = mlflow.pytorch.load_model(MODEL_PATH).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c952d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set model to evaluation mode\n",
    "net.eval()\n",
    "\n",
    "# Collect embeddings and labels from validation set\n",
    "all_embeddings = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img0, img1, _, label0, label1 in val_loader:\n",
    "        img0, img1 = img0.to(DEVICE), img1.to(DEVICE)\n",
    "        label0, label1 = label0.to(DEVICE), label1.to(DEVICE)\n",
    "        output1, output2 = net(img0, img1)\n",
    "\n",
    "        all_embeddings.append(torch.cat([output1, output2]))\n",
    "        all_labels.append(torch.cat([label0, label1]))\n",
    "\n",
    "# Concatenate all embeddings and labels\n",
    "all_embeddings = torch.cat(all_embeddings)  # shape: [N, D]\n",
    "all_labels = torch.cat(all_labels)          # shape: [N]\n",
    "\n",
    "# Normalize embeddings for cosine similarity\n",
    "all_embeddings = F.normalize(all_embeddings, p=2, dim=1)\n",
    "\n",
    "# Group embeddings by class\n",
    "class_to_embeddings = defaultdict(list)\n",
    "for emb, label in zip(all_embeddings, all_labels):\n",
    "    class_to_embeddings[label.item()].append(emb)\n",
    "\n",
    "for key in class_to_embeddings:\n",
    "    class_to_embeddings[key] = torch.stack(class_to_embeddings[key])\n",
    "\n",
    "# Compute intra-class cosine similarities\n",
    "intra_sims = []\n",
    "for emb_list in class_to_embeddings.values():\n",
    "    if len(emb_list) < 2:\n",
    "        continue\n",
    "    for i, j in combinations(range(len(emb_list)), 2):\n",
    "        sim = F.cosine_similarity(emb_list[i].unsqueeze(0), emb_list[j].unsqueeze(0))\n",
    "        intra_sims.append(sim.item())\n",
    "\n",
    "# Compute inter-class cosine similarities\n",
    "inter_sims = []\n",
    "label_keys = list(class_to_embeddings.keys())\n",
    "for i in range(len(label_keys)):\n",
    "    for j in range(i+1, len(label_keys)):\n",
    "        emb_i = class_to_embeddings[label_keys[i]]\n",
    "        emb_j = class_to_embeddings[label_keys[j]]\n",
    "        for e1 in emb_i:\n",
    "            for e2 in emb_j:\n",
    "                sim = F.cosine_similarity(e1.unsqueeze(0), e2.unsqueeze(0))\n",
    "                inter_sims.append(sim.item())\n",
    "\n",
    "# Compute statistics\n",
    "avg_intra_sim = sum(intra_sims) / len(intra_sims)\n",
    "avg_inter_sim = sum(inter_sims) / len(inter_sims)\n",
    "\n",
    "print(f\"✅ Average Intra-Class Cosine Similarity: {avg_intra_sim:.4f}\")\n",
    "print(f\"✅ Average Inter-Class Cosine Similarity: {avg_inter_sim:.4f}\")\n",
    "\n",
    "# Optional: Visualize distributions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(intra_sims, bins=50, alpha=0.5, label='Intra-class CosSim')\n",
    "plt.hist(inter_sims, bins=50, alpha=0.5, label='Inter-class CosSim')\n",
    "plt.title(\"Cosine Similarity Distributions\")\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621c7b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "num = 30\n",
    "# Convert tensors to NumPy arrays\n",
    "X = all_embeddings.cpu().numpy()\n",
    "y = all_labels.cpu().numpy()\n",
    "\n",
    "# Randomly select 5 unique labels\n",
    "unique_labels = np.unique(y)\n",
    "selected_labels = np.random.choice(unique_labels, size=num, replace=False)\n",
    "\n",
    "# Filter data\n",
    "mask = np.isin(y, selected_labels)\n",
    "X_filtered = X[mask]\n",
    "y_filtered = y[mask]\n",
    "\n",
    "# Show counts per selected label before running t-SNE\n",
    "print(\"Sample counts per selected label:\")\n",
    "for label in selected_labels:\n",
    "    count = np.sum(y_filtered == label)\n",
    "    print(f\"Label {label}: {count} samples\")\n",
    "\n",
    "# Dynamically adjust perplexity based on sample count\n",
    "n_samples = X_filtered.shape[0]\n",
    "perplexity = min(30, (n_samples - 1) // 3)  # perplexity < n_samples / 3\n",
    "\n",
    "if perplexity < num:\n",
    "    raise ValueError(f\"Too few samples ({n_samples}) for t-SNE to work reliably with 5 labels.\")\n",
    "\n",
    "# --- t-SNE ---\n",
    "print(f\"Running t-SNE with perplexity={perplexity} on {n_samples} samples...\")\n",
    "tsne = TSNE(n_components=2, perplexity=perplexity, max_iter=1000, random_state=42, metric=\"cosine\")\n",
    "X_tsne = tsne.fit_transform(X_filtered)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=y_filtered, palette='tab10', legend='full', s=30)\n",
    "plt.title(\"t-SNE Visualization of Embeddings (5 Random Labels)\")\n",
    "plt.xlabel(\"t-SNE 1\")\n",
    "plt.ylabel(\"t-SNE 2\")\n",
    "plt.legend(title=\"Class\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08cbb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    # Get one sample (batch of 1)\n",
    "    img1, img2, _, label1, label2 = next(iter(test_loader))\n",
    "\n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        emb1, emb2 = net(img1.to(DEVICE), img2.to(DEVICE))\n",
    "\n",
    "        # Euclidean Distance\n",
    "        euclidean_distance = F.pairwise_distance(emb1, emb2)\n",
    "        cosine_sim = F.cosine_similarity(emb1, emb2)\n",
    "\n",
    "    # Concatenate images side-by-side\n",
    "    concatenated = torch.cat((img1[0], img2[0]), 2)\n",
    "\n",
    "    # Convert labels to list for display\n",
    "    if isinstance(label1, torch.Tensor):\n",
    "        lbl1 = label1[0].cpu().numpy().tolist() if label1.dim() > 0 else label1.item()\n",
    "    else:\n",
    "        lbl1 = label1\n",
    "\n",
    "    if isinstance(label2, torch.Tensor):\n",
    "        lbl2 = label2[0].cpu().numpy().tolist() if label2.dim() > 0 else label2.item()\n",
    "    else:\n",
    "        lbl2 = label2\n",
    "\n",
    "    title = (\n",
    "        f'Label1: {lbl1}\\nLabel2: {lbl2}\\n'\n",
    "        f'Euclidean: {euclidean_distance.item():.2f} | '\n",
    "        f'Cosine Sim: {cosine_sim.item():.2f}'\n",
    "    )\n",
    "\n",
    "    imshow(torchvision.utils.make_grid(concatenated), title)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
